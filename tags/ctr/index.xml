<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Ctr on </title>
    <link>http://luosha865.github.io/tags/ctr/</link>
    <description>Recent content in Ctr on </description>
    <generator>Hugo -- gohugo.io</generator>
    <lastBuildDate>Sat, 03 Aug 2013 00:00:00 +0000</lastBuildDate>
    <atom:link href="http://luosha865.github.io/tags/ctr/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Google在KDD2013上关于CTR的一篇论文</title>
      <link>http://luosha865.github.io/archives/6</link>
      <pubDate>Sat, 03 Aug 2013 00:00:00 +0000</pubDate>
      
      <guid>http://luosha865.github.io/archives/6</guid>
      <description>&lt;p&gt;最近在做CTR，刚好Google在KDD发了一篇文章，讲了他们的一些尝试，总结一下：&lt;/p&gt;

&lt;p&gt;先是一些公式的符号说明：&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://images.cnitblog.com/blog/52809/201308/04011358-7d9b9023e473455e9c21b18486a58fa8.png&#34;&gt;&lt;img style=&#34;background-image: none; padding-top: 0px; padding-left: 0px; display: inline; padding-right: 0px; border-width: 0px;&#34; title=&#34;image&#34; src=&#34;http://images.cnitblog.com/blog/52809/201308/04011358-949b9f116ca94563ab5a0efa047697db.png&#34; alt=&#34;image&#34; width=&#34;447&#34; height=&#34;98&#34; border=&#34;0&#34; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;一、优化算法&lt;/p&gt;

&lt;p&gt;CTR中经常用Logistic regression进行训练，一个常用的Loss Function为&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://images.cnitblog.com/blog/52809/201308/04011358-9e64ee86e7f14dbd8a3da9f8a2bb7c13.png&#34;&gt;&lt;img style=&#34;background-image: none; padding-top: 0px; padding-left: 0px; display: inline; padding-right: 0px; border-width: 0px;&#34; title=&#34;image&#34; src=&#34;http://images.cnitblog.com/blog/52809/201308/04011358-03407ff016d04735b28103abc573d428.png&#34; alt=&#34;image&#34; width=&#34;329&#34; height=&#34;34&#34; border=&#34;0&#34; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Online gradient descent(OGD)是一个常用的优化方法，但是在加上L1正则化后，这种方法不能产生有效的稀疏模型。相比之下 Regularized Dual Averaging (RDA)拥有更好的稀疏性，但是精度不如OGD好。&lt;/p&gt;

&lt;p&gt;FTRL-Proximal 方法可以同时得到稀疏性与精确性，不同于OGD的迭代步骤：&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://images.cnitblog.com/blog/52809/201308/04011358-2e856d4483c64ef0a668ee8c8df2a548.png&#34;&gt;&lt;img style=&#34;background-image: none; padding-top: 0px; padding-left: 0px; display: inline; padding-right: 0px; border-width: 0px;&#34; title=&#34;image&#34; src=&#34;http://images.cnitblog.com/blog/52809/201308/04011359-6f55d3e1f55f4389979bec14102120b5.png&#34; alt=&#34;image&#34; width=&#34;187&#34; height=&#34;39&#34; border=&#34;0&#34; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;其中&lt;img src=&#34;http://sword865.com/blog/wp-content/ql-cache/quicklatex.com-f489909a6f16a3b5a6ee23901d48d9b6_l3.png&#34; class=&#34;ql-img-inline-formula quicklatex-auto-format&#34; alt=&#34;&amp;#92;&amp;#101;&amp;#116;&amp;#97;&amp;#95;&amp;#116;&#34; title=&#34;Rendered by QuickLaTeX.com&#34; height=&#34;12&#34; width=&#34;14&#34; style=&#34;vertical-align: -4px;&#34; /&gt;是一个非增的学习率&lt;/p&gt;

&lt;p&gt;FTRL-Proximal通过下式迭代：&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://images.cnitblog.com/blog/52809/201308/04011359-900ea01f64a547bd99cefe8e375a3a9d.png&#34;&gt;&lt;img style=&#34;background-image: none; padding-top: 0px; padding-left: 0px; display: inline; padding-right: 0px; border-width: 0px;&#34; title=&#34;image&#34; src=&#34;http://images.cnitblog.com/blog/52809/201308/04011359-37b635900b57402a9efc1d0fbf12e7d2.png&#34; alt=&#34;image&#34; width=&#34;482&#34; height=&#34;60&#34; border=&#34;0&#34; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&amp;nbsp;&lt;/p&gt;

&lt;p&gt;其中参数  &lt;img src=&#34;http://sword865.com/blog/wp-content/ql-cache/quicklatex.com-a53d534bdda53a36b558f3ef61ab1d0f_l3.png&#34; class=&#34;ql-img-inline-formula quicklatex-auto-format&#34; alt=&#34;&amp;#92;&amp;#115;&amp;#105;&amp;#103;&amp;#109;&amp;#97;&amp;#95;&amp;#115;&#34; title=&#34;Rendered by QuickLaTeX.com&#34; height=&#34;11&#34; width=&#34;16&#34; style=&#34;vertical-align: -3px;&#34; /&gt;是学习率，一般我们有  &lt;img src=&#34;http://sword865.com/blog/wp-content/ql-cache/quicklatex.com-a4b1e9e9035115afffdd00bcfa2f183c_l3.png&#34; class=&#34;ql-img-inline-formula quicklatex-auto-format&#34; alt=&#34;&amp;#92;&amp;#115;&amp;#117;&amp;#109;&amp;#95;&amp;#123;&amp;#115;&amp;#61;&amp;#49;&amp;#125;&amp;#94;&amp;#116;&amp;#92;&amp;#115;&amp;#105;&amp;#103;&amp;#109;&amp;#97;&amp;#95;&amp;#115;&amp;#61;&amp;#92;&amp;#102;&amp;#114;&amp;#97;&amp;#99;&amp;#123;&amp;#49;&amp;#125;&amp;#123;&amp;#92;&amp;#101;&amp;#116;&amp;#97;&amp;#95;&amp;#116;&amp;#125;&#34; title=&#34;Rendered by QuickLaTeX.com&#34; height=&#34;26&#34; width=&#34;102&#34; style=&#34;vertical-align: -9px;&#34; /&gt;。&lt;/p&gt;

&lt;p&gt;更新公式：&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://images.cnitblog.com/blog/52809/201308/04011359-5cd9cb60ba40409ca3ef8988e2e11f47.png&#34;&gt;&lt;img style=&#34;background-image: none; padding-top: 0px; padding-left: 0px; display: inline; padding-right: 0px; border-width: 0px;&#34; title=&#34;image&#34; src=&#34;http://images.cnitblog.com/blog/52809/201308/04011359-ade11cdad0064b918877e4fbdd0babb1.png&#34; alt=&#34;image&#34; width=&#34;377&#34; height=&#34;74&#34; border=&#34;0&#34; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;算法如下：&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://images.cnitblog.com/blog/52809/201308/04011359-8100f10672694bffa4e1a46fc3b01719.png&#34;&gt;&lt;img style=&#34;background-image: none; padding-top: 0px; padding-left: 0px; display: inline; padding-right: 0px; border: 0px;&#34; title=&#34;image&#34; src=&#34;http://images.cnitblog.com/blog/52809/201308/04011400-cbd594724918403f8c25f0a88ee489ae.png&#34; alt=&#34;image&#34; width=&#34;428&#34; height=&#34;368&#34; border=&#34;0&#34; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;这里多个一个  &lt;img src=&#34;http://sword865.com/blog/wp-content/ql-cache/quicklatex.com-8e8aa5c38c57b55921bdea4b91ec59b3_l3.png&#34; class=&#34;ql-img-inline-formula quicklatex-auto-format&#34; alt=&#34;&amp;#92;&amp;#108;&amp;#97;&amp;#109;&amp;#98;&amp;#100;&amp;#97;&amp;#95;&amp;#50;&#34; title=&#34;Rendered by QuickLaTeX.com&#34; height=&#34;16&#34; width=&#34;17&#34; style=&#34;vertical-align: -3px;&#34; /&gt;是一个L2正则化参数。&lt;/p&gt;

&lt;p&gt;二、学习率&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://sword865.com/blog/wp-content/ql-cache/quicklatex.com-1c291523bb9229304e6ff8843fef467b_l3.png&#34; class=&#34;ql-img-inline-formula quicklatex-auto-format&#34; alt=&#34;&amp;#92;&amp;#100;&amp;#105;&amp;#115;&amp;#112;&amp;#108;&amp;#97;&amp;#121;&amp;#115;&amp;#116;&amp;#121;&amp;#108;&amp;#101;&amp;#32;&amp;#92;&amp;#101;&amp;#116;&amp;#97;&amp;#95;&amp;#116;&amp;#61;&amp;#92;&amp;#102;&amp;#114;&amp;#97;&amp;#99;&amp;#123;&amp;#49;&amp;#125;&amp;#123;&amp;#92;&amp;#115;&amp;#113;&amp;#114;&amp;#116;&amp;#123;&amp;#116;&amp;#125;&amp;#125;&#34; title=&#34;Rendered by QuickLaTeX.com&#34; height=&#34;40&#34; width=&#34;62&#34; style=&#34;vertical-align: -16px;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;由于在求解时，这样，对每一个坐标我们都使用了同样的参数，这样一些没有使用的坐标的参数也会下降，显然这不太合理。&lt;/p&gt;

&lt;p&gt;一个近似最优的选择是：&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://images.cnitblog.com/blog/52809/201308/04011400-7ee5511abe4e40a38bf4950df39c1335.png&#34;&gt;&lt;img style=&#34;background-image: none; padding-top: 0px; padding-left: 0px; margin: 0px; display: inline; padding-right: 0px; border: 0px;&#34; title=&#34;image&#34; src=&#34;http://images.cnitblog.com/blog/52809/201308/04011400-76896e8c7af94f00a204a895b9e243d8.png&#34; alt=&#34;image&#34; width=&#34;244&#34; height=&#34;78&#34; border=&#34;0&#34; /&gt;&lt;/a&gt;g是梯度向量&lt;/p&gt;

&lt;p&gt;&amp;nbsp;&lt;/p&gt;

&lt;p&gt;三、存储空间&lt;/p&gt;

&lt;p&gt;1.特征选择&lt;/p&gt;

&lt;p&gt;在CTR中，跟多特征仅仅出现了一次(In fact, in some of our models, half the unique features occur only once in the entire training set of billions of examples)，这样特征几乎没有什么用，但是存储起来非常浪费空间。L1正则化虽然解决了一些问题，但是这样降低了一些精度，因此另一个选择是&lt;/p&gt;

&lt;p&gt;probabilistic feature inclusion，这种方法中，一个特征第一次出现时，会以一定的概率被保存使用。关于这个概率Google尝试了两种方法：&lt;/p&gt;

&lt;p&gt;Poisson Inclusion：以概率p增加特征，这样一般特征被加入就需要出现1/p次&lt;/p&gt;

&lt;p&gt;Bloom Filter Inclusion：用一系列的Bloom flters来检测特征的前n次出现，一旦检测到出现了n次（因为BF有冲突，所以实际可能少于n），就加入模型并用在后面的训练中。&lt;/p&gt;

&lt;p&gt;2.系数编码&lt;/p&gt;

&lt;p&gt;因为大部分系数都在-2和2之间，因此使用了定点的q2.13编码，同时也保证了小数的一些精度。编码包括一位的符号，2位的整数和13位的小数。&lt;/p&gt;

&lt;p&gt;因此误差可能在OGD算法中发散，因此使用了一个简单的随机取整策略：&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://images.cnitblog.com/blog/52809/201308/04011400-578afca151244861974554d833d64ade.png&#34;&gt;&lt;img style=&#34;background-image: none; padding-top: 0px; padding-left: 0px; display: inline; padding-right: 0px; border: 0px;&#34; title=&#34;image&#34; src=&#34;http://images.cnitblog.com/blog/52809/201308/04011400-eea1a760e2124dcca8efa0e15f16c808.png&#34; alt=&#34;image&#34; width=&#34;285&#34; height=&#34;46&#34; border=&#34;0&#34; /&gt;&lt;/a&gt;R是一个0到1的随机整数。&lt;/p&gt;

&lt;p&gt;3.多个相似模型的训练&lt;/p&gt;

&lt;p&gt;在测试一些超参数的影响时，同时训练多个模型非常有用。观察发现，有些数据可以被多个模型共用，但是另外一些（如系数）不能，如果把模型的系数存在一个HASH表里，就可以让多个变体同时使用这些参数，比如学习率。&lt;/p&gt;

&lt;p&gt;4.单值结构&lt;/p&gt;

&lt;p&gt;有时我们想训练一些模型，他们之间只是删除或增加了一些特征。单值特征为每一个特征存了一个权重，权重被所有有该特征的模型共享，学习方法如下：&lt;/p&gt;

&lt;p&gt;在OGD更新中，每个模型用他自己的的那部分特征计算一个Loss, 然后对每一个特征，每一个模型计算一个新的系数，最后把所有值平均后存为单值。该单值下一步被所有模型使用。&lt;/p&gt;

&lt;p&gt;5.计数与梯度&lt;/p&gt;

&lt;p&gt;假设所有事件包括统一特征的概率相同（一个粗糙但是有效的估计），其中出现了P次，没有出现N次，那么出现的概率就是p=P/(N+P),那么在logistic regression中，正事件的导数是p-1,负事件p，梯度的平方和就是：&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://images.cnitblog.com/blog/52809/201308/04011401-abaa4c9233c443179f17fc62822d599f.png&#34;&gt;&lt;img style=&#34;background-image: none; padding-top: 0px; padding-left: 0px; display: inline; padding-right: 0px; border: 0px;&#34; title=&#34;image&#34; src=&#34;http://images.cnitblog.com/blog/52809/201308/04011401-7fc7cc332f634239bff48a489dd1c997.png&#34; alt=&#34;image&#34; width=&#34;389&#34; height=&#34;145&#34; border=&#34;0&#34; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&amp;nbsp;&lt;/p&gt;

&lt;p&gt;6.采样训练数据：&lt;/p&gt;

&lt;p&gt;CTR中的负样本远高与正样本，因此采样的数据包括满足所有的正样本和部分负样本：&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://images.cnitblog.com/blog/52809/201308/04011401-46a2083bf5904c3c986dcacfc63bb0e2.png&#34;&gt;&lt;img style=&#34;background-image: none; padding-top: 0px; padding-left: 0px; display: inline; padding-right: 0px; border: 0px;&#34; title=&#34;image&#34; src=&#34;http://images.cnitblog.com/blog/52809/201308/04011401-10e2c56d7cf94e08ab2d89e884481391.png&#34; alt=&#34;image&#34; width=&#34;421&#34; height=&#34;64&#34; border=&#34;0&#34; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;在训练中给正样本1的权重，负样本1/r的权重以避免模型结果出错。权重乘如Loss Function对应项中。&lt;/p&gt;

&lt;p&gt;&amp;nbsp;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://images.cnitblog.com/blog/52809/201308/04011401-301af9b06f524c5d947e9739b2e16cf8.png&#34;&gt;&lt;img style=&#34;background-image: none; padding-top: 0px; padding-left: 0px; display: inline; padding-right: 0px; border: 0px;&#34; title=&#34;image&#34; src=&#34;http://images.cnitblog.com/blog/52809/201308/04011401-cf3e0ae967e148a7ae5365ccc0f7fc51.png&#34; alt=&#34;image&#34; width=&#34;349&#34; height=&#34;64&#34; border=&#34;0&#34; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://images.cnitblog.com/blog/52809/201308/04011402-eeef6de793ad4baaae18d7251f6d482d.png&#34;&gt;&lt;img style=&#34;background-image: none; padding-top: 0px; padding-left: 0px; display: inline; padding-right: 0px; border: 0px;&#34; title=&#34;image&#34; src=&#34;http://images.cnitblog.com/blog/52809/201308/04011402-a833a1c25f19478f92c19b6b06db8ed0.png&#34; alt=&#34;image&#34; width=&#34;467&#34; height=&#34;52&#34; border=&#34;0&#34; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;四、模型评价1&lt;/p&gt;

&lt;p&gt;1.进度验证(Progressive Validation)&lt;/p&gt;

&lt;p&gt;因为计算梯度的同时需要计算预测值，因此可以收集这些值。在线的loss反映了算法的表现&amp;#8212;他度量了训练一个数据前得到的预测结果。这样也使得所有数据被同时作用训练集和测试集使用。&lt;/p&gt;

&lt;p&gt;2.可视化加强理解&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://images.cnitblog.com/blog/52809/201308/04011403-ea4fafaae6b5448ca3826eb98a107bf1.png&#34;&gt;&lt;img style=&#34;background-image: none; padding-top: 0px; padding-left: 0px; display: inline; padding-right: 0px; border: 0px;&#34; title=&#34;image&#34; src=&#34;http://images.cnitblog.com/blog/52809/201308/04011404-eaf5de3e66314e82aa675ba92be4fcd8.png&#34; alt=&#34;image&#34; width=&#34;562&#34; height=&#34;267&#34; border=&#34;0&#34; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&amp;nbsp;&lt;/p&gt;

&lt;p&gt;上图对query进行了切片后，将两个模型与一个控制模型模型进行了比较。度量用颜色表示，每行是一个模型，每列是一个切片。&lt;/p&gt;

&lt;p&gt;五、置信估计&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://images.cnitblog.com/blog/52809/201308/04011405-1ba03bff12c740beb22d7c59e6e6d899.png&#34;&gt;&lt;img style=&#34;background-image: none; padding-top: 0px; padding-left: 0px; display: inline; padding-right: 0px; border: 0px;&#34; title=&#34;image&#34; src=&#34;http://images.cnitblog.com/blog/52809/201308/04011405-3f7f56dae9a34817a96f412ad13a8dcd.png&#34; alt=&#34;image&#34; width=&#34;434&#34; height=&#34;119&#34; border=&#34;0&#34; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;六、预测矫正&lt;/p&gt;

&lt;p&gt;矫正的数据&lt;a href=&#34;http://images.cnitblog.com/blog/52809/201308/04011405-372b84501cc84940a965b739cf97a9d9.png&#34;&gt;&lt;img style=&#34;background-image: none; padding-top: 0px; padding-left: 0px; display: inline; padding-right: 0px; border: 0px;&#34; title=&#34;image&#34; src=&#34;http://images.cnitblog.com/blog/52809/201308/04011405-93a76b43c030441c9295ded6239bbb1c.png&#34; alt=&#34;image&#34; width=&#34;71&#34; height=&#34;34&#34; border=&#34;0&#34; /&gt;&lt;/a&gt;p是模型预测的CTR，d是一些训练数据。&lt;/p&gt;

&lt;p&gt;一个常用矫正：[&lt;img style=&#34;background-image: none; padding-top: 0px; padding-left: 0px; margin: 0px; display: inline; padding-right: 0px; border: 0px;&#34; title=&#34;image&#34; src=&#34;http://images.cnitblog.com/blog/52809/201308/04011406-e83e0f3784944b10812b43ca5c74a2c4.png&#34; alt=&#34;image&#34; width=&#34;112&#34; height=&#34;43&#34; border=&#34;0&#34; /&gt;][16]&lt;/p&gt;

&lt;p&gt;两个参数可以用Poisson regression在数据上训练。&lt;/p&gt;

&lt;p style=&#34;margin:0;padding:0;height:1px;overflow:hidden;&#34;&gt;
  &lt;a href=&#34;http://www.wumii.com/widget/relatedItems&#34; style=&#34;border:0;&#34;&gt;&lt;img src=&#34;http://static.wumii.cn/images/pixel.png&#34; alt=&#34;无觅相关文章插件，快速提升流量&#34; style=&#34;border:0;padding:0;margin:0;&#34; /&gt;&lt;/a&gt;
&lt;/p&gt;

&lt;p&gt;[16]: &lt;a href=&#34;http://images.cnitblog.com/blog/52809/201308/04011406-75988e6dea0d4d83b698e96019c9753b.png&#34;&gt;http://images.cnitblog.com/blog/52809/201308/04011406-75988e6dea0d4d83b698e96019c9753b.png&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>