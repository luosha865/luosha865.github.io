<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Ctr on 悟剑阁</title>
    <link>http://luosha865.github.io/tags/ctr/</link>
    <description>Recent content in Ctr on 悟剑阁</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh-CN</language>
    <managingEditor>luosha865@gmail.com (罗刹剑客)</managingEditor>
    <webMaster>luosha865@gmail.com (罗刹剑客)</webMaster>
    <copyright>Copyright (c) 2015. All rights reserved.</copyright>
    <lastBuildDate>Fri, 04 Nov 2016 22:47:21 +0800</lastBuildDate>
    <atom:link href="http://luosha865.github.io/tags/ctr/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>谈谈Factorization Machine</title>
      <link>http://luosha865.github.io/2016/11/04/%E8%B0%88%E8%B0%88Factorization-Machine/</link>
      <pubDate>Fri, 04 Nov 2016 22:47:21 +0800</pubDate>
      <author>luosha865@gmail.com (罗刹剑客)</author>
      <guid>http://luosha865.github.io/2016/11/04/%E8%B0%88%E8%B0%88Factorization-Machine/</guid>
      <description>&lt;p&gt;因子分解机(Factorization Machine, 简称FM)是一种不错的CTR预估模型，也是我们现在在使用的广告点击率预估模型，比起著名的Logistic Regression, FM能够把握一些组合的高阶特征，因此拥有更强的表现力。&lt;/p&gt;

&lt;p&gt;在做点击率预估时，我们的特征往往来自于用户(user)、广告(item)和上下文环境(context)，在线性模型中，这些特征不进行组合的话，就会发生一个很尴尬的情况，因为：&lt;/p&gt;

&lt;div&gt;$$score = f(w_{user} * x_{user} + w_{item} * x_{item} + w_{contex} * x_{contex})$$&lt;/div&gt;

&lt;p&gt;所以，对所有用户&amp;ndash;我们将得到相同的排序。&lt;/p&gt;

&lt;p&gt;因此我们需要引入一些组合特征作为输入模型，然而仅二阶特征组合的可能性就是原始特征的平方之多，但是由于很多特征其实是相互独立的，他们的组合并没有什么价值。FM就是一种能够自动把握一些高阶特征的点击率预估模型，可以自动帮助使用者选择合适的高阶输入。&lt;/p&gt;

&lt;p&gt;我们都知道在矩阵分解的协同过滤中中，我们认为每个用户可以表示成一个K维的特征向量u_k,每个物品也能表示成一个高维向量i_k，这样用户与物品的相关性就可以用两个向量的点击表示，所有用户与所有物品的相关性就可以用两个矩阵相乘的形式表示出来了。&lt;/p&gt;

&lt;p&gt;FM的模型参考了矩阵分解的思想，认为每个特征都可以用一个K维的特征向量表示，那么所有特征之前的相关性就也就可以用两个矩阵的相乘进行表示了，模型表示如下：&lt;/p&gt;

&lt;p&gt;$$y(x) = w&lt;em&gt;0 + \sum&lt;/em&gt;{i=1}^{n} w_i x&lt;em&gt;i } + \sum&lt;/em&gt;{i=1}^{n} \sum&lt;em&gt;{j=i+1}^{n} w&lt;/em&gt;{ij} x_i x_j$$&lt;/p&gt;

&lt;p&gt;FM的优化可以用SGD来做，不过这里推荐带动量（momentum）的min-batch SGD算法，试验中比普通的SGD效果似乎更好。带momentum的SGD模拟了物品运动中的惯性。&lt;/p&gt;

&lt;p&gt;在传统的SGD中：$x&lt;em&gt;{t+1}=x&lt;/em&gt;{t}+Δx&lt;em&gt;{t},Δx&lt;/em&gt;{t}=-ŋg&lt;em&gt;{t}$,其中$g&lt;/em&gt;{t}$代表了梯度。
而在momentum的SGD中：$Δx&lt;em&gt;{t}=px&lt;/em&gt;{t-1}-ŋg_{t}$。&lt;/p&gt;

&lt;p&gt;不过比起LR, FM有一个缺点&amp;ndash;目标函数是非凸的，虽然针对这个缺点也有一些研究比如一些凸函数形式的FM，不过在实践中似乎这并不是一个很严重的问题，合适的初始化方式和优化方法一般是能够给出一个可以接受的解的。&lt;/p&gt;

&lt;p&gt;FM的另外一个缺点是有点耗费内存，对于每个特征都要用一个K维的向量表示导致参数数量是LR的K倍，这方面也是有一些研究的，比如李沐针对这个问题提出的DiFacto就是一个很好的能够降低内存消耗的优化方案。&lt;/p&gt;

&lt;p&gt;最后，还有一种著名的策略是FFM模型，FFM模型被称为FM的升级版，把同一类的特征（比如一些用01向量编码的的离散特征）归到一个field里去，然后要求每个特征在每个field下都有一个不同的K维表示方式，这一下把参数的数量从K变成了FK(F是field的数量)，模型复杂度变的更高了。不过这样做的效果确实不错。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Google在KDD2013上关于CTR的一篇论文</title>
      <link>http://luosha865.github.io/archives/6</link>
      <pubDate>Sat, 03 Aug 2013 00:00:00 +0000</pubDate>
      <author>luosha865@gmail.com (罗刹剑客)</author>
      <guid>http://luosha865.github.io/archives/6</guid>
      <description>&lt;p&gt;最近在做CTR，刚好Google在KDD发了一篇文章，讲了他们的一些尝试，总结一下：&lt;/p&gt;

&lt;p&gt;先是一些公式的符号说明：&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://images.cnitblog.com/blog/52809/201308/04011358-7d9b9023e473455e9c21b18486a58fa8.png&#34;&gt;&lt;img style=&#34;background-image: none; padding-top: 0px; padding-left: 0px; display: inline; padding-right: 0px; border-width: 0px;&#34; title=&#34;image&#34; src=&#34;http://images.cnitblog.com/blog/52809/201308/04011358-949b9f116ca94563ab5a0efa047697db.png&#34; alt=&#34;image&#34; width=&#34;447&#34; height=&#34;98&#34; border=&#34;0&#34; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;一、优化算法&lt;/p&gt;

&lt;p&gt;CTR中经常用Logistic regression进行训练，一个常用的Loss Function为&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://images.cnitblog.com/blog/52809/201308/04011358-9e64ee86e7f14dbd8a3da9f8a2bb7c13.png&#34;&gt;&lt;img style=&#34;background-image: none; padding-top: 0px; padding-left: 0px; display: inline; padding-right: 0px; border-width: 0px;&#34; title=&#34;image&#34; src=&#34;http://images.cnitblog.com/blog/52809/201308/04011358-03407ff016d04735b28103abc573d428.png&#34; alt=&#34;image&#34; width=&#34;329&#34; height=&#34;34&#34; border=&#34;0&#34; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Online gradient descent(OGD)是一个常用的优化方法，但是在加上L1正则化后，这种方法不能产生有效的稀疏模型。相比之下 Regularized Dual Averaging (RDA)拥有更好的稀疏性，但是精度不如OGD好。&lt;/p&gt;

&lt;p&gt;FTRL-Proximal 方法可以同时得到稀疏性与精确性，不同于OGD的迭代步骤：&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://images.cnitblog.com/blog/52809/201308/04011358-2e856d4483c64ef0a668ee8c8df2a548.png&#34;&gt;&lt;img style=&#34;background-image: none; padding-top: 0px; padding-left: 0px; display: inline; padding-right: 0px; border-width: 0px;&#34; title=&#34;image&#34; src=&#34;http://images.cnitblog.com/blog/52809/201308/04011359-6f55d3e1f55f4389979bec14102120b5.png&#34; alt=&#34;image&#34; width=&#34;187&#34; height=&#34;39&#34; border=&#34;0&#34; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;其中$\eta_t$是一个非增的学习率&lt;/p&gt;

&lt;p&gt;FTRL-Proximal通过下式迭代：&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://images.cnitblog.com/blog/52809/201308/04011359-900ea01f64a547bd99cefe8e375a3a9d.png&#34;&gt;&lt;img style=&#34;background-image: none; padding-top: 0px; padding-left: 0px; display: inline; padding-right: 0px; border-width: 0px;&#34; title=&#34;image&#34; src=&#34;http://images.cnitblog.com/blog/52809/201308/04011359-37b635900b57402a9efc1d0fbf12e7d2.png&#34; alt=&#34;image&#34; width=&#34;482&#34; height=&#34;60&#34; border=&#34;0&#34; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&amp;nbsp;&lt;/p&gt;

&lt;p&gt;参数$\sigma$是学习率，一般我们有 ${\sum_{s=1}^t\sigma_s=\frac{1}{\eta_t}}$。&lt;/p&gt;

&lt;p&gt;更新公式：&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://images.cnitblog.com/blog/52809/201308/04011359-5cd9cb60ba40409ca3ef8988e2e11f47.png&#34;&gt;&lt;img style=&#34;background-image: none; padding-top: 0px; padding-left: 0px; display: inline; padding-right: 0px; border-width: 0px;&#34; title=&#34;image&#34; src=&#34;http://images.cnitblog.com/blog/52809/201308/04011359-ade11cdad0064b918877e4fbdd0babb1.png&#34; alt=&#34;image&#34; width=&#34;377&#34; height=&#34;74&#34; border=&#34;0&#34; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;算法如下：&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://images.cnitblog.com/blog/52809/201308/04011359-8100f10672694bffa4e1a46fc3b01719.png&#34;&gt;&lt;img style=&#34;background-image: none; padding-top: 0px; padding-left: 0px; display: inline; padding-right: 0px; border: 0px;&#34; title=&#34;image&#34; src=&#34;http://images.cnitblog.com/blog/52809/201308/04011400-cbd594724918403f8c25f0a88ee489ae.png&#34; alt=&#34;image&#34; width=&#34;428&#34; height=&#34;368&#34; border=&#34;0&#34; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;这里多个一个 $\lambda_2$ 是一个L2正则化参数。&lt;/p&gt;

&lt;p&gt;二、学习率&lt;/p&gt;

&lt;p&gt;$$\displaystyle \eta_t=\frac{1}{\sqrt{t}}$$&lt;/p&gt;

&lt;p&gt;由于在求解时，这样，对每一个坐标我们都使用了同样的参数，这样一些没有使用的坐标的参数也会下降，显然这不太合理。&lt;/p&gt;

&lt;p&gt;一个近似最优的选择是：&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://images.cnitblog.com/blog/52809/201308/04011400-7ee5511abe4e40a38bf4950df39c1335.png&#34;&gt;&lt;img style=&#34;background-image: none; padding-top: 0px; padding-left: 0px; margin: 0px; display: inline; padding-right: 0px; border: 0px;&#34; title=&#34;image&#34; src=&#34;http://images.cnitblog.com/blog/52809/201308/04011400-76896e8c7af94f00a204a895b9e243d8.png&#34; alt=&#34;image&#34; width=&#34;244&#34; height=&#34;78&#34; border=&#34;0&#34; /&gt;&lt;/a&gt;g是梯度向量&lt;/p&gt;

&lt;p&gt;&amp;nbsp;&lt;/p&gt;

&lt;p&gt;三、存储空间&lt;/p&gt;

&lt;p&gt;1.特征选择&lt;/p&gt;

&lt;p&gt;在CTR中，跟多特征仅仅出现了一次(In fact, in some of our models, half the unique features occur only once in the entire training set of billions of examples)，这样特征几乎没有什么用，但是存储起来非常浪费空间。L1正则化虽然解决了一些问题，但是这样降低了一些精度，因此另一个选择是&lt;/p&gt;

&lt;p&gt;probabilistic feature inclusion，这种方法中，一个特征第一次出现时，会以一定的概率被保存使用。关于这个概率Google尝试了两种方法：&lt;/p&gt;

&lt;p&gt;Poisson Inclusion：以概率p增加特征，这样一般特征被加入就需要出现1/p次&lt;/p&gt;

&lt;p&gt;Bloom Filter Inclusion：用一系列的Bloom flters来检测特征的前n次出现，一旦检测到出现了n次（因为BF有冲突，所以实际可能少于n），就加入模型并用在后面的训练中。&lt;/p&gt;

&lt;p&gt;2.系数编码&lt;/p&gt;

&lt;p&gt;因为大部分系数都在-2和2之间，因此使用了定点的q2.13编码，同时也保证了小数的一些精度。编码包括一位的符号，2位的整数和13位的小数。&lt;/p&gt;

&lt;p&gt;因此误差可能在OGD算法中发散，因此使用了一个简单的随机取整策略：&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://images.cnitblog.com/blog/52809/201308/04011400-578afca151244861974554d833d64ade.png&#34;&gt;&lt;img style=&#34;background-image: none; padding-top: 0px; padding-left: 0px; display: inline; padding-right: 0px; border: 0px;&#34; title=&#34;image&#34; src=&#34;http://images.cnitblog.com/blog/52809/201308/04011400-eea1a760e2124dcca8efa0e15f16c808.png&#34; alt=&#34;image&#34; width=&#34;285&#34; height=&#34;46&#34; border=&#34;0&#34; /&gt;&lt;/a&gt;R是一个0到1的随机整数。&lt;/p&gt;

&lt;p&gt;3.多个相似模型的训练&lt;/p&gt;

&lt;p&gt;在测试一些超参数的影响时，同时训练多个模型非常有用。观察发现，有些数据可以被多个模型共用，但是另外一些（如系数）不能，如果把模型的系数存在一个HASH表里，就可以让多个变体同时使用这些参数，比如学习率。&lt;/p&gt;

&lt;p&gt;4.单值结构&lt;/p&gt;

&lt;p&gt;有时我们想训练一些模型，他们之间只是删除或增加了一些特征。单值特征为每一个特征存了一个权重，权重被所有有该特征的模型共享，学习方法如下：&lt;/p&gt;

&lt;p&gt;在OGD更新中，每个模型用他自己的的那部分特征计算一个Loss, 然后对每一个特征，每一个模型计算一个新的系数，最后把所有值平均后存为单值。该单值下一步被所有模型使用。&lt;/p&gt;

&lt;p&gt;5.计数与梯度&lt;/p&gt;

&lt;p&gt;假设所有事件包括统一特征的概率相同（一个粗糙但是有效的估计），其中出现了P次，没有出现N次，那么出现的概率就是p=P/(N+P),那么在logistic regression中，正事件的导数是p-1,负事件p，梯度的平方和就是：&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://images.cnitblog.com/blog/52809/201308/04011401-abaa4c9233c443179f17fc62822d599f.png&#34;&gt;&lt;img style=&#34;background-image: none; padding-top: 0px; padding-left: 0px; display: inline; padding-right: 0px; border: 0px;&#34; title=&#34;image&#34; src=&#34;http://images.cnitblog.com/blog/52809/201308/04011401-7fc7cc332f634239bff48a489dd1c997.png&#34; alt=&#34;image&#34; width=&#34;389&#34; height=&#34;145&#34; border=&#34;0&#34; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&amp;nbsp;&lt;/p&gt;

&lt;p&gt;6.采样训练数据：&lt;/p&gt;

&lt;p&gt;CTR中的负样本远高与正样本，因此采样的数据包括满足所有的正样本和部分负样本：&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://images.cnitblog.com/blog/52809/201308/04011401-46a2083bf5904c3c986dcacfc63bb0e2.png&#34;&gt;&lt;img style=&#34;background-image: none; padding-top: 0px; padding-left: 0px; display: inline; padding-right: 0px; border: 0px;&#34; title=&#34;image&#34; src=&#34;http://images.cnitblog.com/blog/52809/201308/04011401-10e2c56d7cf94e08ab2d89e884481391.png&#34; alt=&#34;image&#34; width=&#34;421&#34; height=&#34;64&#34; border=&#34;0&#34; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;在训练中给正样本1的权重，负样本1/r的权重以避免模型结果出错。权重乘如Loss Function对应项中。&lt;/p&gt;

&lt;p&gt;&amp;nbsp;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://images.cnitblog.com/blog/52809/201308/04011401-301af9b06f524c5d947e9739b2e16cf8.png&#34;&gt;&lt;img style=&#34;background-image: none; padding-top: 0px; padding-left: 0px; display: inline; padding-right: 0px; border: 0px;&#34; title=&#34;image&#34; src=&#34;http://images.cnitblog.com/blog/52809/201308/04011401-cf3e0ae967e148a7ae5365ccc0f7fc51.png&#34; alt=&#34;image&#34; width=&#34;349&#34; height=&#34;64&#34; border=&#34;0&#34; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://images.cnitblog.com/blog/52809/201308/04011402-eeef6de793ad4baaae18d7251f6d482d.png&#34;&gt;&lt;img style=&#34;background-image: none; padding-top: 0px; padding-left: 0px; display: inline; padding-right: 0px; border: 0px;&#34; title=&#34;image&#34; src=&#34;http://images.cnitblog.com/blog/52809/201308/04011402-a833a1c25f19478f92c19b6b06db8ed0.png&#34; alt=&#34;image&#34; width=&#34;467&#34; height=&#34;52&#34; border=&#34;0&#34; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;四、模型评价1&lt;/p&gt;

&lt;p&gt;1.进度验证(Progressive Validation)&lt;/p&gt;

&lt;p&gt;因为计算梯度的同时需要计算预测值，因此可以收集这些值。在线的loss反映了算法的表现&amp;#8212;他度量了训练一个数据前得到的预测结果。这样也使得所有数据被同时作用训练集和测试集使用。&lt;/p&gt;

&lt;p&gt;2.可视化加强理解&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://images.cnitblog.com/blog/52809/201308/04011403-ea4fafaae6b5448ca3826eb98a107bf1.png&#34;&gt;&lt;img style=&#34;background-image: none; padding-top: 0px; padding-left: 0px; display: inline; padding-right: 0px; border: 0px;&#34; title=&#34;image&#34; src=&#34;http://images.cnitblog.com/blog/52809/201308/04011404-eaf5de3e66314e82aa675ba92be4fcd8.png&#34; alt=&#34;image&#34; width=&#34;562&#34; height=&#34;267&#34; border=&#34;0&#34; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&amp;nbsp;&lt;/p&gt;

&lt;p&gt;上图对query进行了切片后，将两个模型与一个控制模型模型进行了比较。度量用颜色表示，每行是一个模型，每列是一个切片。&lt;/p&gt;

&lt;p&gt;五、置信估计&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://images.cnitblog.com/blog/52809/201308/04011405-1ba03bff12c740beb22d7c59e6e6d899.png&#34;&gt;&lt;img style=&#34;background-image: none; padding-top: 0px; padding-left: 0px; display: inline; padding-right: 0px; border: 0px;&#34; title=&#34;image&#34; src=&#34;http://images.cnitblog.com/blog/52809/201308/04011405-3f7f56dae9a34817a96f412ad13a8dcd.png&#34; alt=&#34;image&#34; width=&#34;434&#34; height=&#34;119&#34; border=&#34;0&#34; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;六、预测矫正&lt;/p&gt;

&lt;p&gt;矫正的数据&lt;a href=&#34;http://images.cnitblog.com/blog/52809/201308/04011405-372b84501cc84940a965b739cf97a9d9.png&#34;&gt;&lt;img style=&#34;background-image: none; padding-top: 0px; padding-left: 0px; display: inline; padding-right: 0px; border: 0px;&#34; title=&#34;image&#34; src=&#34;http://images.cnitblog.com/blog/52809/201308/04011405-93a76b43c030441c9295ded6239bbb1c.png&#34; alt=&#34;image&#34; width=&#34;71&#34; height=&#34;34&#34; border=&#34;0&#34; /&gt;&lt;/a&gt;p是模型预测的CTR，d是一些训练数据。&lt;/p&gt;

&lt;p&gt;一个常用矫正：&lt;a href=&#34;http://images.cnitblog.com/blog/52809/201308/04011406-75988e6dea0d4d83b698e96019c9753b.png&#34;&gt;&lt;img style=&#34;background-image: none; padding-top: 0px; padding-left: 0px; margin: 0px; display: inline; padding-right: 0px; border: 0px;&#34; title=&#34;image&#34; src=&#34;http://images.cnitblog.com/blog/52809/201308/04011406-e83e0f3784944b10812b43ca5c74a2c4.png&#34; alt=&#34;image&#34; width=&#34;112&#34; height=&#34;43&#34; border=&#34;0&#34; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;两个参数可以用Poisson regression在数据上训练。&lt;/p&gt;

&lt;p style=&#34;margin:0;padding:0;height:1px;overflow:hidden;&#34;&gt;
  &lt;a href=&#34;http://www.wumii.com/widget/relatedItems&#34; style=&#34;border:0;&#34;&gt;&lt;img src=&#34;http://static.wumii.cn/images/pixel.png&#34; alt=&#34;无觅相关文章插件，快速提升流量&#34; style=&#34;border:0;padding:0;margin:0;&#34; /&gt;&lt;/a&gt;
&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>